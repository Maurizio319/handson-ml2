{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to generate a multilayer network with single nodes in the layers and backpropagation mechanism\n",
    "\n",
    "$a^{(L-1)} \\rightarrow a^{(L)}$ with weight and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(pred, true) -> float:\n",
    "    return (pred - true) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use gradient descent\n",
    "def gradient_descent(factor, correction, learning_rate):\n",
    "    new_f = factor - learning_rate * correction\n",
    "    return new_f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_fun(z):\n",
    "    \"\"\"Sigmoid function\n",
    "\n",
    "    Args:\n",
    "        z (float): Number resulting from combination of activations and weights\n",
    "    \"\"\"\n",
    "    print(\"z\", z)\n",
    "    a = float(1 / (1 + np.exp(-z)))\n",
    "    print(\"a\", a)\n",
    "    return a\n",
    "\n",
    "\n",
    "# sigmoid_fun(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(\n",
    "        self,\n",
    "        position,\n",
    "        prev,\n",
    "        bias,\n",
    "        weights: list,\n",
    "        layer: int,\n",
    "        final: bool = False,\n",
    "        activ_function= sigmoid_fun,\n",
    "    ) -> None:\n",
    "        self.layer = layer\n",
    "        self.position = position\n",
    "        self.final = final\n",
    "        self.prev = prev\n",
    "        self.bias = bias\n",
    "        self.activ_function=activ_function\n",
    "\n",
    "        self.set_weights(weights=weights)\n",
    "        self.set_z()\n",
    "        self.set_activation()\n",
    "\n",
    "    def set_weights(self, weights: list):\n",
    "        self.weights = weights\n",
    "\n",
    "    def set_z(self):\n",
    "        if self.layer == 0:\n",
    "            assert type(self.prev) == int or type(self.prev) == float\n",
    "            self.z = self.prev\n",
    "\n",
    "        else:\n",
    "            weight = self.weights[self.prev.position]\n",
    "            act_prev = self.prev.act\n",
    "            print(\"weight\", weight)\n",
    "            print(\"act_prev\", act_prev)\n",
    "            print(\"bias\", self.bias)\n",
    "\n",
    "            # Calculate z\n",
    "            self.z = weight * act_prev + self.bias\n",
    "            print(self.z)\n",
    "\n",
    "    def set_activation(self):\n",
    "        if self.layer == 0:\n",
    "            self.act = self.z\n",
    "\n",
    "        else:\n",
    "            self.act = self.activ_function(self.z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight 0.2312797920920377\n",
      "act_prev 2\n",
      "bias 0\n",
      "0.4625595841840754\n",
      "z 0.4625595841840754\n",
      "a 0.6136212047278341\n"
     ]
    }
   ],
   "source": [
    "a_1 = Neuron(position=0, prev=2, bias=1, layer=0, weights=[0.2312797920920377])\n",
    "a_2 = Neuron(position=0, prev=a_1, bias=0, layer=1, weights=[0.2312797920920377])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You know that\n",
    "\n",
    "$$\n",
    "a^{(L)} = w^{(L)} a^{(L-1)} + b^{(L)} = \\sigma(z^{(L)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have to compute\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C_0}{\\partial w^{(L)}} = \n",
    "\\frac{\\partial C_0}{\\partial a^{(L)}}\n",
    "\\frac{\\partial a^{(L)}}{\\partial z^{(L)}}\n",
    "\\frac{\\partial z^{(L)}}{\\partial w^{(L)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = 0.7\n",
    "learning_rate = 0.4\n",
    "\n",
    "d_ca = lambda a_l, y: 2 * (a_l - y)\n",
    "d_az = lambda z: float(np.exp(- z) / (1 + np.exp(- z))**2)\n",
    "d_aw = lambda node: node.prev.act\n",
    "d_cw = d_ca(a_2.act, y_true) * d_az(a_2.z) * d_aw(a_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23276730837283383"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(factor=0.2, correction=d_cw, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Layer:\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         bias,\n",
    "#         weights,\n",
    "#         index: int,\n",
    "#         pop: int,\n",
    "#         final: bool = False,\n",
    "#         initial_input = np.array([])\n",
    "#     ) -> None:\n",
    "#         self.initial_input = initial_input\n",
    "#         self.final = final\n",
    "#         self.index = index\n",
    "#         self.pop = pop\n",
    "#         self.bias = bias\n",
    "#         self.weights = weights\n",
    "\n",
    "#         if len(self.initial_input) == 0:\n",
    "#             self.initial_input = False\n",
    "\n",
    "#     def calculate_z(self):\n",
    "#         # Take previous layer\n",
    "#         layer_prev = \n",
    "\n",
    "#     def calculate_a(self):\n",
    "#         pass\n",
    "\n",
    "#         ## Get nodes\n",
    "#         # self.get_neurons()\n",
    "\n",
    "#     # def get_neurons(self):\n",
    "#     #     self.neurons = [Neuron(position=pos, layer=self.index, final=self.final) for pos in range(self.pop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias = 1\n",
    "# n_layers = 2\n",
    "# pop = 1\n",
    "\n",
    "# initial_input = np.array([2])\n",
    "# all_weights = [np.array([]), np.array(0.2)]\n",
    "\n",
    "# all_layers = []\n",
    "# for lay in range(n_layers):\n",
    "#     if lay == 0:\n",
    "#         layer = Layer(\n",
    "#             bias=bias,\n",
    "#             pop=pop,\n",
    "#             initial_input=initial_input,\n",
    "#             weights=all_weights[lay],\n",
    "#             index=lay,\n",
    "#         )\n",
    "#     elif lay == n_layers - 1:\n",
    "#         layer = Layer(\n",
    "#             bias=bias, pop=pop, final=True, weights=all_weights[lay], index=lay\n",
    "#         )\n",
    "#     else:\n",
    "#         layer = Layer(bias=bias, pop=pop, weights=all_weights[lay], index=lay)\n",
    "\n",
    "#     all_layers.append(layer)\n",
    "\n",
    "# # [Layer(bias=1, weights=weights[lay], index=lay) for lay in range()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ca = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning_practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
