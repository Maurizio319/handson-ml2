{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to generate a multilayer network with single nodes in the layers and backpropagation mechanism\n",
    "\n",
    "$a^{(L-1)} \\rightarrow a^{(L)}$ with weight and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "y_true = 0.7\n",
    "learning_rate = 0.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(pred, true) -> float:\n",
    "    return (pred - true) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use gradient descent\n",
    "def gradient_descent(factor, correction, learning_rate):\n",
    "    new_f = factor - learning_rate * correction\n",
    "    return new_f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid_fun(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(\n",
    "        self,\n",
    "        position,\n",
    "        neuralnetw,\n",
    "        # weights: list,\n",
    "        layer: int,\n",
    "        final: bool = False,\n",
    "        activ_function=sigmoid_fun,\n",
    "    ) -> None:\n",
    "        self.neuralnetw = neuralnetw\n",
    "        self.layer = layer\n",
    "        self.position = position\n",
    "        self.final = final\n",
    "        self.activ_function = activ_function\n",
    "\n",
    "        # self.get_previous(neuralnetw)\n",
    "        # self.get_weights(weights=weights)\n",
    "        # self.set_z()\n",
    "        # self.set_activation()\n",
    "\n",
    "    def get_previous(self, neuralnetw):\n",
    "        pass\n",
    "\n",
    "    def get_previous_layer_weights(self):\n",
    "        pass\n",
    "\n",
    "    # def set_z(self):\n",
    "    #     if self.layer == 0:\n",
    "    #         assert type(self.prev) == int or type(self.prev) == float\n",
    "    #         self.z = self.prev\n",
    "\n",
    "    #     else:\n",
    "    #         weight = self.weights[self.prev.position]\n",
    "    #         act_prev = self.prev.act\n",
    "    #         print(\"weight\", weight)\n",
    "    #         print(\"act_prev\", act_prev)\n",
    "    #         print(\"bias\", self.bias)\n",
    "\n",
    "    #         # Calculate z\n",
    "    #         self.z = weight * act_prev + self.bias\n",
    "    #         print(\"z value\", self.z)\n",
    "\n",
    "    def set_activation(self):\n",
    "        if self.layer == 0:\n",
    "            self.act = self.z\n",
    "\n",
    "        else:\n",
    "            self.act = self.activ_function(self.z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetw:\n",
    "    \"\"\"Neural Network used to collect new neurons\"\"\"\n",
    "\n",
    "    def __init__(self, disposition: tuple) -> None:\n",
    "        self.n_layers = len(disposition)\n",
    "        self.nodes = sum(disposition)\n",
    "\n",
    "        self.layers = []\n",
    "        lay_idx = 0\n",
    "\n",
    "        for layer in disposition:\n",
    "            self.layers.append([])\n",
    "\n",
    "        for layer in disposition:\n",
    "            if lay_idx == len(disposition) - 1:  # if last layer\n",
    "                final = True\n",
    "            else:\n",
    "                final = False\n",
    "\n",
    "            for el in range(layer):\n",
    "                nw_neuron = Neuron(\n",
    "                    position=el,\n",
    "                    neuralnetw=self,\n",
    "                    layer=lay_idx,\n",
    "                    final=final,\n",
    "                    activ_function=sigmoid_fun,\n",
    "                )\n",
    "                self.layers[lay_idx].append(nw_neuron)\n",
    "            lay_idx += 1\n",
    "\n",
    "    def set_input(self, input, verbose=False):\n",
    "        assert len(input) == len(self.layers[0])\n",
    "\n",
    "        if verbose:\n",
    "            for node_idx in range(len(self.layers[0])):\n",
    "                node = self.layers[0][node_idx]\n",
    "                node.z = input[node_idx]\n",
    "                node.act = node.z\n",
    "\n",
    "            print(f\"input {input} given to input layer {self.layers[0]}\")\n",
    "\n",
    "    def set_weights(self, w_mat, index_layer, verbose=False):\n",
    "        if index_layer == 0:\n",
    "            raise ValueError(\n",
    "                \"The first layer cannot be assigned weights, The weights refear to the connections between the previous layer and the one indicated\"\n",
    "            )\n",
    "\n",
    "        layer_of_interest = self.layers[index_layer]\n",
    "        prev_layer = self.layers[index_layer - 1]\n",
    "        assert (\n",
    "            w_mat.shape == (len(layer_of_interest), len(prev_layer))\n",
    "        ), f\"w_mat should be {(len(layer_of_interest), len(prev_layer))}, not {w_mat.shape}\"\n",
    "\n",
    "        for node in layer_of_interest:\n",
    "            node.weights = w_mat[node.position]\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"Node {node} of layer {index_layer} in position {node.position} gets weights {w_mat[node.position]}\"\n",
    "                )\n",
    "\n",
    "    def set_bias(self, bias, index_layer, verbose=False):\n",
    "        if index_layer == 0:\n",
    "            raise ValueError(\n",
    "                \"The first layer cannot be assigned bias, as it is the input layer\"\n",
    "            )\n",
    "\n",
    "        layer_of_interest = self.layers[index_layer]\n",
    "        for node in layer_of_interest:\n",
    "            node.bias = bias  # assign bias to all nodes\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"All nodes {layer_of_interest} were assigned the bias {bias}\")\n",
    "\n",
    "    def get_z(self, index_layer, position, verbose=False):\n",
    "        if index_layer != 0:\n",
    "            node = self.layers[index_layer][position]\n",
    "\n",
    "            node.z = 0\n",
    "            string = \"\"\n",
    "            for prev_node_idx in range(len(node.weights)):\n",
    "                wgt = node.weights[prev_node_idx]\n",
    "                activation = self.layers[index_layer - 1][prev_node_idx].act\n",
    "                if verbose:\n",
    "                    string += f\"{wgt}*{activation} + \"\n",
    "\n",
    "                node.z += wgt * activation\n",
    "            bias = node.bias\n",
    "            node.z += bias\n",
    "            string += f\"{bias}\"\n",
    "\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"node layer {index_layer} position {position} - z was assigned to value {string} = {node.z}\"\n",
    "                )\n",
    "\n",
    "    def get_act(self, index_layer, position, activ_function=\"identity\"):\n",
    "\n",
    "        if activ_function == \"identity\":\n",
    "            self.layers[index_layer][position].act = self.layers[index_layer][position].z\n",
    "\n",
    "        elif index_layer != 0:\n",
    "            self.layers[index_layer][position].act = activ_function(\n",
    "                self.layers[index_layer][position].z\n",
    "            )\n",
    "\n",
    "    def calculate_error(self, pred, true):\n",
    "        error =(pred - true)**2 \n",
    "        print(f\"Error = {error}\")\n",
    "        return error\n",
    "\n",
    "    def backprop(self, true_value):\n",
    "        last_node = self.layers[-1][0]\n",
    "        # At first calculate error on last node (single)\n",
    "        error = self.calculate_error(pred = last_node.act, true=true_value)\n",
    "\n",
    "        # calculate with respect to all elements\n",
    "        d_caL = 2 * (last_node.act - true_value)\n",
    "\n",
    "        layer = 1\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3]\n",
    "b = [2, 3, 4]\n",
    "c = [1]\n",
    "d = [1,2,3,4,5,6]\n",
    "ll = [a, b, c, d]\n",
    "mx = False\n",
    "\n",
    "\n",
    "def find_max_length(ll, condition=\"even\"):\n",
    "    mx = False\n",
    "    for l in ll:\n",
    "        if (condition == \"odd\" and len(l) % 2 == 1) or (\n",
    "            condition == \"even\" and len(l) % 2 == 0\n",
    "        ):\n",
    "            if mx:\n",
    "                if mx < len(l):\n",
    "                    mx = len(l)\n",
    "            else:\n",
    "                mx = len(l)\n",
    "\n",
    "    return mx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = find_max_length(ll, \"even\") + find_max_length(ll, \"odd\")\n",
    "max\n",
    "\n",
    "scroller = 0\n",
    "string= \"\"\n",
    "ll_copy = ll.copy()\n",
    "while scroller < \n",
    "for l in ll_copy:\n",
    "    if len(l) == positions - scroller:\n",
    "        string += l.pop(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4 % 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1 --> 2\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"0\\n1 --> 2\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_fun(z):\n",
    "    \"\"\"Sigmoid function\n",
    "\n",
    "    Args:\n",
    "        z (float): Number resulting from combination of activations and weights\n",
    "    \"\"\"\n",
    "    # print(\"z\", z)\n",
    "    a = float(1 / (1 + np.exp(-z)))\n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input [10, 20] given to input layer [<__main__.Neuron object at 0x79dc221faae0>, <__main__.Neuron object at 0x79dc221f8ec0>]\n",
      "Node <__main__.Neuron object at 0x79dc221faa80> of layer 1 in position 0 gets weights [0.1 0.2]\n",
      "Node <__main__.Neuron object at 0x79dc221f8c20> of layer 1 in position 1 gets weights [0.3 0.4]\n",
      "Node <__main__.Neuron object at 0x79dc221fb6e0> of layer 2 in position 0 gets weights [0.5 0.6]\n",
      "All nodes [<__main__.Neuron object at 0x79dc221faa80>, <__main__.Neuron object at 0x79dc221f8c20>] were assigned the bias 1\n",
      "All nodes [<__main__.Neuron object at 0x79dc221fb6e0>] were assigned the bias 1\n",
      "node layer 1 position 0 - z was assigned to value 0.1*10 + 0.2*20 + 1 = 6.0\n",
      "node layer 1 position 1 - z was assigned to value 0.3*10 + 0.4*20 + 1 = 12.0\n",
      "node layer 2 position 0 - z was assigned to value 0.5*6.0 + 0.6*12.0 + 1 = 11.2\n"
     ]
    }
   ],
   "source": [
    "a = NeuralNetw(disposition=(2, 2, 1))\n",
    "a.set_input([10, 20], verbose=True)\n",
    "a.set_weights(w_mat=np.array([[0.1, 0.2], [0.3, 0.4]]), index_layer=1, verbose=True)\n",
    "a.set_weights(w_mat=np.array([[0.5, 0.6]]), index_layer=2, verbose=True)\n",
    "a.set_bias(bias=1, index_layer=1, verbose=True)\n",
    "a.set_bias(bias=1, index_layer=2, verbose=True)\n",
    "\n",
    "# a.get_z(index_layer=1, position=0, verbose=True)\n",
    "# a.get_act(index_layer=1, position=0, activ_function=sigmoid_fun)\n",
    "# a.get_z(index_layer=1, position=1, verbose=True)\n",
    "# a.get_act(index_layer=1, position=1, activ_function=sigmoid_fun)\n",
    "# a.get_z(index_layer=2, position=0, verbose=True)\n",
    "# a.get_act(index_layer=2, position=0, activ_function=sigmoid_fun)\n",
    "\n",
    "for i in range(1, len(a.layers)):\n",
    "    for position in range(len(a.layers[i])):\n",
    "        a.get_z(index_layer=i, position=position, verbose=True)\n",
    "        a.get_act(index_layer=i, position=position, activ_function=\"identity\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error = 14.440000000000005\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(14.440000000000005)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate error\n",
    "pred = a.layers[-1][0].act\n",
    "a.calculate_error(pred=pred, true=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Neuron' object has no attribute 'prev'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[147], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprev\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Neuron' object has no attribute 'prev'"
     ]
    }
   ],
   "source": [
    "a.layers[1][0].prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set functions to compute derivatives\n",
    "d_zw = lambda node: node.prev.act if isinstance(node.prev, Neuron) else node.prev\n",
    "d_az = lambda z: float(np.exp(-z) / (1 + np.exp(-z)) ** 2)\n",
    "d_ca = lambda a_l, y: 2 * (a_l - y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight 0.2577738696148034\n",
      "act_prev 2\n",
      "bias 0\n",
      "0.5155477392296068\n",
      "z 0.5155477392296068\n",
      "a 0.6261060908703497\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "a_1 = Neuron(position=0, prev=2, bias=1, layer=0, weights=[0.2577738696148034])\n",
    "a_2 = Neuron(position=0, prev=a_1, bias=0, layer=1, weights=[0.2577738696148034])\n",
    "\n",
    "# Compute loss gradient with respect to activation (output neuron)\n",
    "dA_2 = d_ca(a_2.act, y_true)\n",
    "\n",
    "# Backpropagate through a_2 (output layer neuron)\n",
    "dA_1 = a_2.backward(dA_2)\n",
    "\n",
    "# Backpropagate through a_1 (hidden layer neuron, if necessary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You know that\n",
    "\n",
    "$$\n",
    "a^{(L)} = w^{(L)} a^{(L-1)} + b^{(L)} = \\sigma(z^{(L)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have to compute\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C_0}{\\partial w^{(L)}} = \n",
    "\\frac{\\partial C_0}{\\partial a^{(L)}}\n",
    "\\frac{\\partial a^{(L)}}{\\partial z^{(L)}}\n",
    "\\frac{\\partial z^{(L)}}{\\partial w^{(L)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.06919344481261032"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_cw = d_ca(a_2.act, y_true) * d_az(a_2.z) * d_aw(a_2)\n",
    "d_cw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2577738696148034"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(\n",
    "    factor=0.2312797920920377, correction=d_cw, learning_rate=learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Layer:\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         bias,\n",
    "#         weights,\n",
    "#         index: int,\n",
    "#         pop: int,\n",
    "#         final: bool = False,\n",
    "#         initial_input = np.array([])\n",
    "#     ) -> None:\n",
    "#         self.initial_input = initial_input\n",
    "#         self.final = final\n",
    "#         self.index = index\n",
    "#         self.pop = pop\n",
    "#         self.bias = bias\n",
    "#         self.weights = weights\n",
    "\n",
    "#         if len(self.initial_input) == 0:\n",
    "#             self.initial_input = False\n",
    "\n",
    "#     def calculate_z(self):\n",
    "#         # Take previous layer\n",
    "#         layer_prev =\n",
    "\n",
    "#     def calculate_a(self):\n",
    "#         pass\n",
    "\n",
    "#         ## Get nodes\n",
    "#         # self.get_neurons()\n",
    "\n",
    "#     # def get_neurons(self):\n",
    "#     #     self.neurons = [Neuron(position=pos, layer=self.index, final=self.final) for pos in range(self.pop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias = 1\n",
    "# n_layers = 2\n",
    "# pop = 1\n",
    "\n",
    "# initial_input = np.array([2])\n",
    "# all_weights = [np.array([]), np.array(0.2)]\n",
    "\n",
    "# all_layers = []\n",
    "# for lay in range(n_layers):\n",
    "#     if lay == 0:\n",
    "#         layer = Layer(\n",
    "#             bias=bias,\n",
    "#             pop=pop,\n",
    "#             initial_input=initial_input,\n",
    "#             weights=all_weights[lay],\n",
    "#             index=lay,\n",
    "#         )\n",
    "#     elif lay == n_layers - 1:\n",
    "#         layer = Layer(\n",
    "#             bias=bias, pop=pop, final=True, weights=all_weights[lay], index=lay\n",
    "#         )\n",
    "#     else:\n",
    "#         layer = Layer(bias=bias, pop=pop, weights=all_weights[lay], index=lay)\n",
    "\n",
    "#     all_layers.append(layer)\n",
    "\n",
    "# # [Layer(bias=1, weights=weights[lay], index=lay) for lay in range()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning_practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
